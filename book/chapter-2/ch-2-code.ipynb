{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Kaggle work book for personal learning following along with fast.ai's book, chapter 2,\nfound here: https://github.com/fastai/fastbook/blob/master/02_production.ipynb","metadata":{}},{"cell_type":"code","source":"#NB: Kaggle requires phone verification to use the internet or a GPU. If you haven't done that yet, the cell below will fail\n#    This code is only here to check that your internet is enabled. It doesn't do anything else.\n#    Here's a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367\n\n\nimport socket,warnings\ntry:\n    socket.setdefaulttimeout(1)\n    socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect(('1.1.1.1', 53))\nexcept socket.error as ex: raise Exception(\"STOP: No internet. If on Kaggle: Click '>|' in top right and set 'Internet' switch to on. Else maybe check that the file is trusted. \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It's a good idea to ensure you're running the latest version of any libraries you \n# need. Ifwe're on Kaggle we can do this for the virtual machine environment Kaggle has\n# for us, since might not be up to date. `!pip install -Uqq <libraries>` upgrades to the\n# latest version of <libraries>, q for quiet\n# NB: You can safely ignore any warnings or errors pip spits out about running as root\n# or incompatibilities\nimport os\non_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif on_kaggle:\n    # Update fastai if we're on Kaggle as it might be out of date\n    !pip install -Uqq fastai\n# duckduckgo_search: convinience library for getting images using Duck Duck Go search\n#  Github: https://github.com/deedy5/duckduckgo_search\n!pip install -Uqq duckduckgo_search","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from duckduckgo_search import ddg_images\n# fastai.vision.all must be imported before fastcore.all on Intel Macs. Kills kernels\n# otherwise, don't know why (as of Aug. 2022, might have a fix later)\nfrom fastai.vision.all import *\nfrom fastcore.all import *\nfrom fastdownload import download_url","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport re\nimport json\n# Can't import fastbook on my Mac, so copying the utils function from fastbook to here\n# https://github.com/fastai/fastbook/blob/b7f756b49d4eb0d3ce96c0c29be98f4f293cde9f/utils.py#L45\ndef search_images_ddg(key,max_n=200):\n     \"\"\"Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images\n        (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api)\n     \"\"\"\n     url        = 'https://duckduckgo.com/'\n     params     = {'q':key}\n     res        = requests.post(url,data=params)\n     searchObj  = re.search(r'vqd=([\\d-]+)\\&',res.text)\n     if not searchObj: print('Token Parsing Failed !'); return\n     requestUrl = url + 'i.js'\n     headers    = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'}\n     params     = (('l','us-en'),('o','json'),('q',key),('vqd',searchObj.group(1)),('f',',,,'),('p','1'),('v7exp','a'))\n     urls       = []\n     while True:\n         try:\n             res  = requests.get(requestUrl,headers=headers,params=params)\n             data = json.loads(res.text)\n             for obj in data['results']:\n                 urls.append(obj['image'])\n                 max_n = max_n - 1\n                 if max_n < 1: return L(set(urls))     # dedupe\n             if 'next' not in data: return L(set(urls))\n             requestUrl = url + data['next']\n         except:\n             pass\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspect at search_images_ddg","metadata":{}},{"cell_type":"code","source":"search_images_ddg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download one bear image.","metadata":{}},{"cell_type":"code","source":"grizzly_bear_urls = search_images_ddg('grizzly bear', max_n=1)\nlen(grizzly_bear_urls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"destination = 'data/grizzly.jpg'\n\ndownload_url(grizzly_bear_urls[0], destination, show_progress=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspect the image.","metadata":{}},{"cell_type":"code","source":"image = Image.open(destination)\nimage.to_thumb(128,128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use fastai's `download_images` to download images for our search terms. Put each category\nin a folder:","metadata":{}},{"cell_type":"code","source":"bear_types = 'grizzly','black','teddy'\n\nfrom pathlib import Path\npath = Path('data/bears')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import sleep\nif not path.exists():\n    path.mkdir()\n    max_n = 10\n    for o in bear_types:\n        destination = (path/o)\n        destination.mkdir(exist_ok=True)\n        print(f'Searching for {max_n} {o} bear images')\n        results = search_images_ddg(f'{o} bear', max_n=max_n)\n        print(f'Downloading {max_n} {o} bear images')\n        download_images(destination, urls=results)\n        print(f'Done downloading {max_n} {o} bear images')\n        print('Sleeping 10 seconds')\n        sleep(10)  # Pause to avoid over-loading server or rate limits\n        print('Done sleeping')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check folders have image files.","metadata":{}},{"cell_type":"code","source":"fns = get_image_files(path)\nfns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delete images we can't open.","metadata":{}},{"cell_type":"code","source":"failed = verify_images(fns)\nfailed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In Jupyter Notebooks the value returned by the last expression is printed out as a\n# string. If we don't care to see that output, we can supress it with a semi-colon at\n# the end of the line\nfailed.map(Path.unlink);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an example of how to use the DataBlock. DataLoaders need DataBlocks.\nThe DataBlock has a .dataloaders attribute you can call to get DataLoaders.\nDataLoaders object has typically two, but can have more, DataLoader objects. One for\ntraining and one for validation.","metadata":{}},{"cell_type":"code","source":"bears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128, ResizeMethod.Squish))\n\n# Some Options for Resize:\n# Resize(128) This crops\n# Resize(128, ResizeMethod.Squish)\n# Resize(128, ResizeMethod.Pad, pad_mode='zeros')\n\nbear_dataloaders = bears.dataloaders(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspect some images.","metadata":{}},{"cell_type":"code","source":"# Note: we're grabbing images from the validation set, seen here by calling\n# bear_dataloaders.valid\nbear_dataloaders.valid.show_batch(max_n=4, nrows=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another good option for item transforms is RandomResizedCrop","metadata":{}},{"cell_type":"code","source":"bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\nbear_dataloaders = bears.dataloaders(path)\n# Note: we're grabbing images from the training set, seen here by calling\n# bear_dataloaders.train\nbear_dataloaders.train.show_batch(max_n=4, nrows=1, unique=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When `unique=True`, the same image is repeated with different calls to `RandomResizedCrop`. aka data augmentation. These augmentations happen to the images\nwhen the images are read from disk. It changes the image, passes it in for training\nand then forgets about it. The images don't get written back out or saved anywhere with the transform that was performed on it.","metadata":{}},{"cell_type":"markdown","source":"For natual images (real life images of things), fast.ai provides a standard set of\naugmentations that have been found to work pretty well. aug_transforms(). When all\nthe images are the same size you can apply transforms to a batch all at the same\ntime using the GPU. Saving time over one by one and/or running on the CPU.\n\nAugmentations doubled to show more clearly upon inspection what's going on for the\nlesson.","metadata":{}},{"cell_type":"code","source":"bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\nbear_dataloaders = bears.dataloaders(path)\nbear_dataloaders.train.show_batch(max_n=8, nrows=2, unique=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suggestion from book: Train your model first, then clean your data. Instead of\nclean, then train.","metadata":{}},{"cell_type":"markdown","source":"Actual training time. Using RandomResizedCrop on each item and aug_transforms() on\nthe batch.","metadata":{}},{"cell_type":"code","source":"bears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms(mult=10)\n    )\ndls = bears.dataloaders(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a learner and train it. In this case, we will just use fine-tune since it's a\npre-trained model.","metadata":{}},{"cell_type":"code","source":"learn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see how well the model is doing, we can look at a confusion matrix.","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Useful to see where errors are occuring. One way is to show things with the higest\nloss from the models predictions.\n\nplot_top_losses() will have labels that show:\nPrediction/Actual/Loss/Probability\n- Probability is the confidence level from 0 to 1 that the model has assigned to its\nprediction. They're percentages. So 0.2 is 20% confidence. 0.98 is 98% confidence.","metadata":{}},{"cell_type":"code","source":"interp.plot_top_losses(5, nrows=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tying back to the suggestion above, training first before cleaning, you can then use\nthe model to help find data issues you need to clean and deal with and do it\nquickly.","metadata":{}},{"cell_type":"markdown","source":"fast.ai has a handy GUI for helping to clean (Image) data called\n`ImageClassifierCleaner`.","metadata":{}},{"cell_type":"code","source":"cleaner = ImageClassifierCleaner(learn)\ncleaner\n# Images will be ordered by loss. So ones showing up first at the front of the list\n# are more likely to be the ones with issues.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cleaner doesn't actually delete or change labels directly, it just returns the\nindexes of things to change. So use the following code to change and delete things.","metadata":{}},{"cell_type":"code","source":"for idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run again on cleaned data. Maybe train again as well.","metadata":{}},{"cell_type":"markdown","source":"Export model for inference.","metadata":{}},{"cell_type":"code","source":"# Export model\nmodel_path = Path('models')\nmodel_path.mkdir(exist_ok=True, parents=True)\nlearn.export(model_path/'resnet18.pkl')","metadata":{},"execution_count":null,"outputs":[]}]}