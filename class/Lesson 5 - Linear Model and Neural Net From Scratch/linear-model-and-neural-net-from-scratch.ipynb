{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Copied and edited from https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch.\n\nA fast.ai course taught by Jeremy Howard.\n\nEdited for personal learning while going through the course myself.\n\nPairs with class video, [lesson 5](https://www.youtube.com/watch?v=_rXzeWq4C6w&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU) from 2022.","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook we're going to build and train a deep learning model \"from scratch\" -- by which I mean that we're not going to use any pre-built architecture, or optimizers, or data loading frameworks, etc.\n\nWe'll be assuming you already know the basics of how a neural network works. If you don't, read this notebook first: [How does a neural net really work?\n](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work). We'll be using Kaggle's [Titanic](https://www.kaggle.com/competitions/titanic/) competition in this notebook, because it's very small and simple, but also has displays many of the tricky real-life issues that we need to handle in most practical projects. (Note, however, that this competition is a small \"learner\" competition on Kaggle, so don't expect to actually see much benefits from using a neural net just yet; that will come once we try our some real competitions!)\n\nIt's great to be able to run the same notebook on your own machine or Colab, as well as Kaggle. To allow for this, we use this code to download the data as needed when not on Kaggle (see [this notebook](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners/) for details about this technique):","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Kaggle sets an environment variable and we can check for it to know if we're on Kaggle\non_kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n# A notebook that's part of the Titanic competition on Kaggle will already have the data\n# downloaded and unzipped for you\nif on_kaggle: path = Path('../input/titanic')\nelse:\n    # Make sure the kaggle pip package is installed\n    !pip install -Uqq kaggle\n    path = Path('data')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:31.383381Z","iopub.execute_input":"2022-09-07T20:32:31.383860Z","iopub.status.idle":"2022-09-07T20:32:31.420538Z","shell.execute_reply.started":"2022-09-07T20:32:31.383798Z","shell.execute_reply":"2022-09-07T20:32:31.419420Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Note that the data for Kaggle comps always lives in the `../input` folder. The easiest way to get the path is to click the \"K\" button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\n\nWe'll be using *numpy* and *pytorch* for array calculations in this notebook, and *pandas* for working with tabular data, so we'll import them and set them to display using a bit more space than they default to.","metadata":{"hidden":true}},{"cell_type":"code","source":"import torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:31.425791Z","iopub.execute_input":"2022-09-07T20:32:31.426037Z","iopub.status.idle":"2022-09-07T20:32:32.005095Z","shell.execute_reply.started":"2022-09-07T20:32:31.426002Z","shell.execute_reply":"2022-09-07T20:32:32.004299Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning the data","metadata":{"heading_collapsed":true}},{"cell_type":"markdown","source":"This is a *tabular data* competition -- the data is in the form of a table. It's provided as a Comma Separated Values (CSV) file. We can open it using the *pandas* library, which will create a `DataFrame`.","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe = pd.read_csv(path/'train.csv')\ntraining_dataframe","metadata":{"hidden":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.006359Z","iopub.execute_input":"2022-09-07T20:32:32.007248Z","iopub.status.idle":"2022-09-07T20:32:32.040114Z","shell.execute_reply.started":"2022-09-07T20:32:32.007208Z","shell.execute_reply":"2022-09-07T20:32:32.039272Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"     PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket  \\\n0              1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   \n1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599   \n2              3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   \n3              4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803   \n4              5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   \n..           ...       ...     ...                                                ...     ...   ...    ...    ...               ...   \n886          887         0       2                              Montvila, Rev. Juozas    male  27.0      0      0            211536   \n887          888         1       1                       Graham, Miss. Margaret Edith  female  19.0      0      0            112053   \n888          889         0       3           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1      2        W./C. 6607   \n889          890         1       1                              Behr, Mr. Karl Howell    male  26.0      0      0            111369   \n890          891         0       3                                Dooley, Mr. Patrick    male  32.0      0      0            370376   \n\n        Fare Cabin Embarked  \n0     7.2500   NaN        S  \n1    71.2833   C85        C  \n2     7.9250   NaN        S  \n3    53.1000  C123        S  \n4     8.0500   NaN        S  \n..       ...   ...      ...  \n886  13.0000   NaN        S  \n887  30.0000   B42        S  \n888  23.4500   NaN        S  \n889  30.0000  C148        C  \n890   7.7500   NaN        Q  \n\n[891 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Montvila, Rev. Juozas</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>211536</td>\n      <td>13.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Graham, Miss. Margaret Edith</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112053</td>\n      <td>30.0000</td>\n      <td>B42</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>W./C. 6607</td>\n      <td>23.4500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Behr, Mr. Karl Howell</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>111369</td>\n      <td>30.0000</td>\n      <td>C148</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Dooley, Mr. Patrick</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>370376</td>\n      <td>7.7500</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã— 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"As we learned in the *How does a neural net really work* notebook, we going to want to multiply each column by some coefficients. But we can see in the `Cabin` column that there are `NaN` values, which is how Pandas refers to missing values. We can't multiply something by a missing value!\n\nLet's check which columns contain `NaN` values. Pandas' `isna()` function returns `True` (which is treated as `1` when used as a number) for `NaN` values, so we can just add them up for each column:","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe.isna().sum()","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.042521Z","iopub.execute_input":"2022-09-07T20:32:32.042903Z","iopub.status.idle":"2022-09-07T20:32:32.052184Z","shell.execute_reply.started":"2022-09-07T20:32:32.042868Z","shell.execute_reply":"2022-09-07T20:32:32.051107Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Notice that by default Pandas sums over columns.\n\nWe'll need to replace the missing values with something. It doesn't generally matter too much what we choose. We'll use the most common value (the \"*mode*\"). We can use the `mode` function for that. One wrinkle is that it returns more than one row in the case of ties, so we just grab the first row with `iloc[0]`:","metadata":{"hidden":true}},{"cell_type":"code","source":"modes = training_dataframe.mode().iloc[0]\nmodes","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.053352Z","iopub.execute_input":"2022-09-07T20:32:32.054025Z","iopub.status.idle":"2022-09-07T20:32:32.073294Z","shell.execute_reply.started":"2022-09-07T20:32:32.053990Z","shell.execute_reply":"2022-09-07T20:32:32.072545Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"PassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"BTW, it's never a good idea to use functions without understanding them. So be sure to google for anything you're not familiar with. E.g if you want to learn about `iloc` (which is a very important function indeed!) then Google will give you a link to a [great tutorial](https://www.shanelynn.ie/pandas-iloc-loc-select-rows-and-columns-dataframe/).\n\nNow that we've got the mode of each column, we can use `fillna` to replace the missing values with the mode of each column. We'll do it \"in place\" -- meaning that we'll change the dataframe itself, rather than returning a new one.","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe.fillna(modes, inplace=True)","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.074642Z","iopub.execute_input":"2022-09-07T20:32:32.074900Z","iopub.status.idle":"2022-09-07T20:32:32.084407Z","shell.execute_reply.started":"2022-09-07T20:32:32.074866Z","shell.execute_reply":"2022-09-07T20:32:32.083537Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can now check there's no missing values left:","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe.isna().sum()","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.085715Z","iopub.execute_input":"2022-09-07T20:32:32.086237Z","iopub.status.idle":"2022-09-07T20:32:32.097979Z","shell.execute_reply.started":"2022-09-07T20:32:32.086195Z","shell.execute_reply":"2022-09-07T20:32:32.097191Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"PassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"That was essentially the most simple way to fill missing values in the data. There are other ways. Why do it this way?\nBeacuse most of the time, it doesn't make that much of a difference. All the data and all the possible combinations of\nvalues that can be had, you're usually filling just a small number of values that have bad data with something, and\noverall, it won't impact the overall outcome. Generally, make a decision for your own circumstance.\n\nAlso this is targeted at just being the baseline first model. You can make it more complicated later if you need to get\nbetter results.\n\nDiscard vs fill: Fill. Missing data just removes things and the model performs better with more. Fast.ai will\nautomatically create a new column and it will just be a boolean to show if a row had missing data or not.","metadata":{}},{"cell_type":"markdown","source":"Here's how we get a quick summary of all the numeric columns in the dataset:","metadata":{"hidden":true}},{"cell_type":"code","source":"import numpy as np\n\ntraining_dataframe.describe(include=(np.number))","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.099534Z","iopub.execute_input":"2022-09-07T20:32:32.099925Z","iopub.status.idle":"2022-09-07T20:32:32.132713Z","shell.execute_reply.started":"2022-09-07T20:32:32.099886Z","shell.execute_reply":"2022-09-07T20:32:32.131999Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       PassengerId    Survived      Pclass         Age       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642   28.566970    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071   13.199572    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000   22.000000    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000   24.000000    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000   35.000000    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000   80.000000    8.000000    6.000000  512.329200","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>28.566970</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>13.199572</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>35.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that `Fare` contains mainly values of around `0` to `30`, but there's a few really big ones. This is very common with fields that contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\n\nYou can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don't forget: if you're not entirely sure what a histogram is, Google \"[histogram tutorial](https://www.google.com/search?q=histogram+tutorial&oq=histogram+tutorial)\" and do a bit of reading before continuing on):","metadata":{"execution":{"iopub.execute_input":"2022-05-13T11:02:34.328433Z","iopub.status.busy":"2022-05-13T11:02:34.327999Z","iopub.status.idle":"2022-05-13T11:02:34.336993Z","shell.execute_reply":"2022-05-13T11:02:34.335466Z","shell.execute_reply.started":"2022-05-13T11:02:34.32838Z"},"hidden":true}},{"cell_type":"code","source":"# X: Fare cost\n# Y: Count with that fare\ntraining_dataframe['Fare'].hist(legend=True);","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.134068Z","iopub.execute_input":"2022-09-07T20:32:32.134477Z","iopub.status.idle":"2022-09-07T20:32:32.374678Z","shell.execute_reply.started":"2022-09-07T20:32:32.134439Z","shell.execute_reply":"2022-09-07T20:32:32.373915Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWTklEQVR4nO3db4xc1X3G8e8T28SUpV5sh5XltbKOsEAIJwY2ZFH+aBcrBEhi84KiRFZxkKvNCxI5KlIxqdQoUl84iigFWqGuCqpp3GzIH7KWQ1tTwwhFChA7cbBhQ72hdtkV8RJjttmAKSa/vpiz7rBZs7Pz13vm+Uijuffcc+ee3zA8c33mzqwiAjMzy8t7mj0AMzOrPYe7mVmGHO5mZhlyuJuZZcjhbmaWoYXNHgDA8uXLo6urq6J9f/e733HeeefVdkBnsVaqt5Vqhdaq17XWxv79+38TEe+badtZEe5dXV3s27evon0LhQK9vb21HdBZrJXqbaVaobXqda21IenombZ5WsbMLEMOdzOzDDnczcwydFbMuZuZVeOtt95idHSUkydPNnsof2DJkiUMDw9X9RiLFy+ms7OTRYsWlb2Pw93M5r3R0VHOP/98urq6kNTs4bzDb3/7W84///yK948Ijh8/zujoKKtXry57P0/LmNm8d/LkSZYtW3bWBXstSGLZsmVz/leJw93MspBjsE+ppDaHu5lZhjznbmbZ6dr2o5o+3pHtn561z4IFC1i7du3p9R/+8IdU+s37Wpj34X5wbIIv1Pg/ZLnK+Q9uZq3h3HPP5cCBA3PaJyKICN7zntpPonhaxsysDiYnJ1m/fj0f//jHWbt2LUNDQwAcOXKEiy++mFtuuYXLLruMl156iW9+85t8+MMf5oMf/CBf+9rXanL8eX/mbmZ2NnjjjTdYt24dAKtXr+a73/0ujzzyCJJ488036enpYcOGDQAcPnyYHTt20NPTw549ezh8+DDPPPMMEcGGDRt48skn+cQnPlHVeBzuZmY1MH1a5q233uKrX/0qhUKBhQsXMjY2xrFjxwB4//vfT09PDwB79uxhz549XH755UDxjP/w4cMOdzOzs9HOnTt55ZVXePLJJ1m6dCldXV2nr1Uv/QngiODOO+/ki1/8Yk2P7zl3M7M6mJiY4MILL2TRokU88cQTHD0686/zfupTn+LBBx9kcnISgLGxMcbHx6s+/qxn7pIuBr5T0vQB4K+Ah1J7F3AEuDkiTqh4tf09wA3A68AXIuJnVY/UzKxMZ8OVbJs2beKzn/0sPT09XHXVVVxyySUz9rv22msZHh7m6quvBqCtrY1vfetbXHjhhVUdf9Zwj4gXgHUAkhYAY8AjwDZgb0Rsl7Qtrd8BXA+sSbePAPenezOzbE2deU9Zvnw5P/nJT2b8bZlDhw69Y33r1q1s3bq1puOZ67TMeuBXEXEU2AjsSO07gBvT8kbgoSh6CmiXtKIWgzUzs/IoIsrvLD0I/Cwi/k7SaxHRntoFnIiIdkm7ge0R8eO0bS9wR0Tsm/ZY/UA/QEdHx5WDg4MVFTD+6gTH3qho16qtXbmk4cecnJykra2t4cdthlaqFVqr3lrXumTJEi666KKaPV4tvf322yxYsKDqxxkZGWFiYuIdbX19ffsjonum/mVfLSPpHGADcOf0bRERksp/lyjuMwAMAHR3d0elf2Pwvp1D3HWwORf9HNnU2/Bj+m9P5quV6q11rcPDw7S1tZ2VPx5W7U/+QvGKmsWLF5++XLIcc5mWuZ7iWfuxtH5sarol3U99vDsGrCrZrzO1mZnVxeLFizl+/DhzmYmYL6Z+z33x4sVz2m8up7yfB75dsr4L2AxsT/dDJe1fkjRI8YPUiYh4eU6jMjObg87OTkZHR3nllVeaPZQ/cPLkyTkH83RTf4lpLsoKd0nnAZ8ESq+y3w48LGkLcBS4ObU/SvEyyBGKl0LeOqcRmZnN0aJFi+b0V4oaqVAozGk6pVbKCveI+B2wbFrbcYpXz0zvG8BtNRmdmZlVxN9QNTPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczswyVFe6S2iV9T9IvJQ1LulrSUkmPSTqc7i9IfSXpXkkjkp6VdEV9SzAzs+nKPXO/B/i3iLgE+BAwDGwD9kbEGmBvWge4HliTbv3A/TUdsZmZzWrWcJe0BPgE8ABARPxvRLwGbAR2pG47gBvT8kbgoSh6CmiXtKLG4zYzs3ehiHj3DtI6YAB4nuJZ+35gKzAWEe2pj4ATEdEuaTewPSJ+nLbtBe6IiH3THref4pk9HR0dVw4ODlZUwPirExx7o6Jdq7Z25ZKGH3NycpK2traGH7cZWqlWaK16XWtt9PX17Y+I7pm2LSxj/4XAFcCXI+JpSffw/1MwAERESHr3d4lpImKA4psG3d3d0dvbO5fdT7tv5xB3HSynjNo7sqm34ccsFApU+lzNN61UK7RWva61/sqZcx8FRiPi6bT+PYphf2xquiXdj6ftY8Cqkv07U5uZmTXIrOEeEb8GXpJ0cWpaT3GKZhewObVtBobS8i7glnTVTA8wEREv13bYZmb2bsqdz/gysFPSOcCLwK0U3xgelrQFOArcnPo+CtwAjACvp75mZtZAZYV7RBwAZpq0Xz9D3wBuq25YZmZWDX9D1cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDJUVrhLOiLpoKQDkvaltqWSHpN0ON1fkNol6V5JI5KelXRFPQswM7M/NJcz976IWBcRU38oexuwNyLWAHvTOsD1wJp06wfur9VgzcysPNVMy2wEdqTlHcCNJe0PRdFTQLukFVUcx8zM5kgRMXsn6b+AE0AA/xARA5Jei4j2tF3AiYhol7Qb2B4RP07b9gJ3RMS+aY/ZT/HMno6OjisHBwcrKmD81QmOvVHRrlVbu3JJw485OTlJW1tbw4/bDK1UK7RWva61Nvr6+vaXzKa8w8IyH+NjETEm6ULgMUm/LN0YESFp9neJd+4zAAwAdHd3R29v71x2P+2+nUPcdbDcMmrryKbehh+zUChQ6XM137RSrdBa9brW+itrWiYixtL9OPAIcBVwbGq6Jd2Pp+5jwKqS3TtTm5mZNcis4S7pPEnnTy0D1wKHgF3A5tRtMzCUlncBt6SrZnqAiYh4ueYjNzOzMypnPqMDeKQ4rc5C4F8i4t8k/RR4WNIW4Chwc+r/KHADMAK8Dtxa81Gbmdm7mjXcI+JF4EMztB8H1s/QHsBtNRmdmZlVxN9QNTPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczswyVHe6SFkj6uaTdaX21pKcljUj6jqRzUvt70/pI2t5Vp7GbmdkZzOXMfSswXLL+DeDuiLgIOAFsSe1bgBOp/e7Uz8zMGqiscJfUCXwa+Me0LuAa4Hupyw7gxrS8Ma2Ttq9P/c3MrEHKPXP/W+AvgN+n9WXAaxFxKq2PAivT8krgJYC0fSL1NzOzBlk4WwdJnwHGI2K/pN5aHVhSP9AP0NHRQaFQqOhxOs6F29eemr1jHVQ65mpMTk425bjN0Eq1QmvV61rrb9ZwBz4KbJB0A7AY+GPgHqBd0sJ0dt4JjKX+Y8AqYFTSQmAJcHz6g0bEADAA0N3dHb29vRUVcN/OIe46WE4ZtXdkU2/Dj1koFKj0uZpvWqlWaK16XWv9zTotExF3RkRnRHQBnwMej4hNwBPATanbZmAoLe9K66Ttj0dE1HTUZmb2rqq5zv0O4M8ljVCcU38gtT8ALEvtfw5sq26IZmY2V3Oaz4iIAlBIyy8CV83Q5yTwJzUYm5mZVcjfUDUzy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDI0a7hLWizpGUm/kPScpK+n9tWSnpY0Iuk7ks5J7e9N6yNpe1edazAzs2nKOXN/E7gmIj4ErAOuk9QDfAO4OyIuAk4AW1L/LcCJ1H536mdmZg00a7hH0WRaXZRuAVwDfC+17wBuTMsb0zpp+3pJqtWAzcxsdoqI2TtJC4D9wEXA3wPfBJ5KZ+dIWgX8a0RcJukQcF1EjKZtvwI+EhG/mfaY/UA/QEdHx5WDg4MVFTD+6gTH3qho16qtXbmk4cecnJykra2t4cdthlaqFVqrXtdaG319ffsjonumbQvLeYCIeBtYJ6kdeAS4pNpBRcQAMADQ3d0dvb29FT3OfTuHuOtgWWXU3JFNvQ0/ZqFQoNLnar5ppVqhtep1rfU3p6tlIuI14AngaqBd0lSqdgJjaXkMWAWQti8BjtdisGZmVp5yrpZ5XzpjR9K5wCeBYYohf1PqthkYSsu70jpp++NRztyPmZnVTDnzGSuAHWne/T3AwxGxW9LzwKCkvwZ+DjyQ+j8A/LOkEeBV4HN1GLeZmb2LWcM9Ip4FLp+h/UXgqhnaTwJ/UpPRmZlZRfwNVTOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8vQrOEuaZWkJyQ9L+k5SVtT+1JJj0k6nO4vSO2SdK+kEUnPSrqi3kWYmdk7lXPmfgq4PSIuBXqA2yRdCmwD9kbEGmBvWge4HliTbv3A/TUftZmZvatZwz0iXo6In6Xl3wLDwEpgI7AjddsB3JiWNwIPRdFTQLukFbUeuJmZnZkiovzOUhfwJHAZ8N8R0Z7aBZyIiHZJu4HtEfHjtG0vcEdE7Jv2WP0Uz+zp6Oi4cnBwsKICxl+d4NgbFe1atbUrlzT8mJOTk7S1tTX8uM3QSrVCa9XrWmujr69vf0R0z7RtYbkPIqkN+D7wlYj4n2KeF0VESCr/XaK4zwAwANDd3R29vb1z2f20+3YOcdfBssuoqSObeht+zEKhQKXP1XzTSrVCa9XrWuuvrKtlJC2iGOw7I+IHqfnY1HRLuh9P7WPAqpLdO1ObmZk1SDlXywh4ABiOiL8p2bQL2JyWNwNDJe23pKtmeoCJiHi5hmM2M7NZlDOf8VHgT4GDkg6ktq8C24GHJW0BjgI3p22PAjcAI8DrwK21HLCZmc1u1nBPH4zqDJvXz9A/gNuqHJeZmVXB31A1M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDM0a7pIelDQu6VBJ21JJj0k6nO4vSO2SdK+kEUnPSrqinoM3M7OZlXPm/k/AddPatgF7I2INsDetA1wPrEm3fuD+2gzTzMzmYuFsHSLiSUld05o3Ar1peQdQAO5I7Q9FRABPSWqXtCIiXq7ZiM8iXdt+1PBj3r721Okn3szsTCqdc+8oCexfAx1peSXwUkm/0dRmZmYNNOuZ+2wiIiTFXPeT1E9x6oaOjg4KhUJFx+84t3g22yo6zqXi52q+mZycbJlaobXqda31V2m4H5uabpG0AhhP7WPAqpJ+nantD0TEADAA0N3dHb29vRUN5L6dQ9x1sOr3qHnj9rWnuLnC52q+KRQKVPq6mI9aqV7XWn+VTsvsAjan5c3AUEn7LemqmR5gItf5djOzs9msp7ySvk3xw9PlkkaBrwHbgYclbQGOAjen7o8CNwAjwOvArXUYs5mZzaKcq2U+f4ZN62foG8Bt1Q7KzMyq42+ompllyOFuZpYhh7uZWYYc7mZmGXK4m5llyOFuZpYhh7uZWYYc7mZmGXK4m5llyOFuZpYhh7uZWYYc7mZmGXK4m5llyOFuZpah1vkTRhlpxh/mnnJk+6ebdmwzK5/P3M3MMuRwNzPLkMPdzCxDDnczsww53M3MMlSXq2UkXQfcAywA/jEittfjONY6mnWFkK8Osvmq5uEuaQHw98AngVHgp5J2RcTztT6WWb018k3l9rWn+ELJ8fzGYtWox5n7VcBIRLwIIGkQ2Ag43DPQzLAzq5d6vq5nex3X601cEVHbB5RuAq6LiD9L638KfCQivjStXz/Qn1YvBl6o8JDLgd9UuO981Er1tlKt0Fr1utbaeH9EvG+mDU37hmpEDAAD1T6OpH0R0V2DIc0LrVRvK9UKrVWva62/elwtMwasKlnvTG1mZtYg9Qj3nwJrJK2WdA7wOWBXHY5jZmZnUPNpmYg4JelLwL9TvBTywYh4rtbHKVH11M4800r1tlKt0Fr1utY6q/kHqmZm1nz+hqqZWYYc7mZmGZrX4S7pOkkvSBqRtK3Z46mWpAcljUs6VNK2VNJjkg6n+wtSuyTdm2p/VtIVzRv53ElaJekJSc9Lek7S1tSea72LJT0j6Rep3q+n9tWSnk51fSddhICk96b1kbS9q6kFVEDSAkk/l7Q7redc6xFJByUdkLQvtTX1tTxvw73kZw6uBy4FPi/p0uaOqmr/BFw3rW0bsDci1gB70zoU616Tbv3A/Q0aY62cAm6PiEuBHuC29N8v13rfBK6JiA8B64DrJPUA3wDujoiLgBPAltR/C3Aitd+d+s03W4HhkvWcawXoi4h1Jde0N/e1HBHz8gZcDfx7yfqdwJ3NHlcN6uoCDpWsvwCsSMsrgBfS8j8An5+p33y8AUMUf48o+3qBPwJ+BnyE4jcXF6b2069pilebXZ2WF6Z+avbY51BjJ8VAuwbYDSjXWtO4jwDLp7U19bU8b8/cgZXASyXro6ktNx0R8XJa/jXQkZazqT/9M/xy4GkyrjdNUxwAxoHHgF8Br0XEqdSltKbT9abtE8Cyhg64On8L/AXw+7S+jHxrBQhgj6T96adVoMmvZf+B7HkkIkJSVteuSmoDvg98JSL+R9LpbbnVGxFvA+sktQOPAJc0d0T1IekzwHhE7JfU2+ThNMrHImJM0oXAY5J+WbqxGa/l+Xzm3io/c3BM0gqAdD+e2ud9/ZIWUQz2nRHxg9Scbb1TIuI14AmKUxPtkqZOskprOl1v2r4EON7YkVbso8AGSUeAQYpTM/eQZ60ARMRYuh+n+MZ9FU1+Lc/ncG+VnznYBWxOy5spzk1Ptd+SPnnvASZK/gl41lPxFP0BYDgi/qZkU671vi+dsSPpXIqfLwxTDPmbUrfp9U49DzcBj0eaoD3bRcSdEdEZEV0U/798PCI2kWGtAJLOk3T+1DJwLXCIZr+Wm/1BRJUfYtwA/CfFucu/bPZ4alDPt4GXgbcozsNtoTj3uBc4DPwHsDT1FcWrhX4FHAS6mz3+Odb6MYrzlM8CB9Lthozr/SDw81TvIeCvUvsHgGeAEeC7wHtT++K0PpK2f6DZNVRYdy+wO+daU12/SLfnprKo2a9l//yAmVmG5vO0jJmZnYHD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MM/R+LGI35V6FlmgAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"To fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable. Note, however, that there are zeros in the `Fare` column, and `log(0)` is infinite -- to fix this, we'll simply add `1` to all values first:","metadata":{"execution":{"iopub.execute_input":"2022-05-13T11:02:34.328433Z","iopub.status.busy":"2022-05-13T11:02:34.327999Z","iopub.status.idle":"2022-05-13T11:02:34.336993Z","shell.execute_reply":"2022-05-13T11:02:34.335466Z","shell.execute_reply.started":"2022-05-13T11:02:34.32838Z"},"hidden":true}},{"cell_type":"code","source":"training_dataframe['LogFare'] = np.log(training_dataframe['Fare']+1)","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.375865Z","iopub.execute_input":"2022-09-07T20:32:32.376641Z","iopub.status.idle":"2022-09-07T20:32:32.382834Z","shell.execute_reply.started":"2022-09-07T20:32:32.376582Z","shell.execute_reply":"2022-09-07T20:32:32.382032Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The histogram now shows a more even distribution of values without the long tail:","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe['LogFare'].hist();","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.384461Z","iopub.execute_input":"2022-09-07T20:32:32.384829Z","iopub.status.idle":"2022-09-07T20:32:32.605140Z","shell.execute_reply.started":"2022-09-07T20:32:32.384785Z","shell.execute_reply":"2022-09-07T20:32:32.604421Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAATz0lEQVR4nO3df4xlZ33f8fcH28DWQ70gu6PtetW1hJuKeBVjj4wRVTRji8RAVDtSioxcsImrTSUnAmXVeuEfoCmSq9ZQoVCrG5Z4CYSJZbBY2Satu3jk+g/H7DrG6x/QbGEpHpndEtZrBlxHa779Y84m42XGM/fO3Llzn75f0tXc85wf9/vo3vnsuc8852yqCklSW14z7AIkSWvPcJekBhnuktQgw12SGmS4S1KDzh52AQDnn39+bd++va99f/rTn3LuueeubUHrzD5sDKPeh1GvH+xDrw4dOvSjqrpgsXUbIty3b9/OwYMH+9p3ZmaGycnJtS1ondmHjWHU+zDq9YN96FWS7y+1zmEZSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KBlwz3J65M8muRbSZ5K8omu/c4k30vyePe4tGtPks8kOZLkiSSXDbgPkqQzrGSe+0vAVVU1l+Qc4OEkX+/W/euquvuM7d8FXNw93gbc0f2UJK2TZc/ca95ct3hO93i1m8BfC3yh2+8RYHOSLasvVZK0UlnJf9aR5CzgEPBm4LNVdWuSO4G3M39mfwDYXVUvJbkXuK2qHu72PQDcWlUHzzjmTmAnwPj4+OXT09N9dWBubo6xsbG+9t0oRqkPh2dPLto+vgmOvTi4192x9bzBHbwzSu/DYka9frAPvZqamjpUVROLrVvR7Qeq6mXg0iSbgXuSXAJ8BPgh8FpgD3Ar8G9XWlRV7en2Y2Jiovq9XNfLldfXTbvvW7R9145T3H54cHezOHrD5MCOfdoovQ+LGfX6wT6spZ5my1TV88CDwDVV9Vw39PIS8MfAFd1ms8C2Bbtd2LVJktbJSmbLXNCdsZNkE/BO4Nunx9GTBLgOeLLbZT/wgW7WzJXAyap6bgC1S5KWsJLv0VuAfd24+2uAu6rq3iTfSHIBEOBx4F91298PvBs4AvwM+OCaVy1JelXLhntVPQG8dZH2q5bYvoBbVl+aJKlfXqEqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGLRvuSV6f5NEk30ryVJJPdO0XJfmLJEeS/FmS13btr+uWj3Trtw+4D5KkM6zkzP0l4Kqq+hXgUuCaJFcC/x74dFW9GTgB3NxtfzNwomv/dLedJGkdLRvuNW+uWzynexRwFXB3174PuK57fm23TLf+6iRZq4IlSctLVS2/UXIWcAh4M/BZ4D8Aj3Rn5yTZBny9qi5J8iRwTVU92637X8DbqupHZxxzJ7ATYHx8/PLp6em+OjA3N8fY2Fhf+24Uo9SHw7MnF20f3wTHXhzc6+7Yet7gDt4ZpfdhMaNeP9iHXk1NTR2qqonF1p29kgNU1cvApUk2A/cA/2S1RVXVHmAPwMTERE1OTvZ1nJmZGfrdd6MYpT7ctPu+Rdt37TjF7YdX9HHqy9EbJgd27NNG6X1YzKjXD/ZhLfU0W6aqngceBN4ObE5y+rf5QmC2ez4LbAPo1p8H/PVaFCtJWpmVzJa5oDtjJ8km4J3AM8yH/G91m90IfK17vr9bplv/jVrJ2I8kac2s5Hv0FmBfN+7+GuCuqro3ydPAdJJ/B/wlsLfbfi/wJ0mOAD8Grh9A3ZKkV7FsuFfVE8BbF2n/LnDFIu3/F/jna1KdJKkvXqEqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KBlwz3JtiQPJnk6yVNJPtS1fzzJbJLHu8e7F+zzkSRHknwnya8PsgOSpF909gq2OQXsqqrHkrwBOJTkgW7dp6vqPy7cOMlbgOuBXwb+IfDfk/zjqnp5LQuXJC1t2TP3qnquqh7rnv8EeAbY+iq7XAtMV9VLVfU94AhwxVoUK0lamVTVyjdOtgMPAZcAvw/cBLwAHGT+7P5Ekj8EHqmqL3b77AW+XlV3n3GsncBOgPHx8cunp6f76sDc3BxjY2N97btRjFIfDs+eXLR9fBMce3Fwr7tj63mDO3hnlN6HxYx6/WAfejU1NXWoqiYWW7eSYRkAkowBXwE+XFUvJLkD+AOgup+3A7+90uNV1R5gD8DExERNTk6udNdXmJmZod99N4pR6sNNu+9btH3XjlPcfnjFH6eeHb1hcmDHPm2U3ofFjHr9YB/W0opmyyQ5h/lg/1JVfRWgqo5V1ctV9XPgj/i7oZdZYNuC3S/s2iRJ62Qls2UC7AWeqapPLWjfsmCz3wSe7J7vB65P8rokFwEXA4+uXcmSpOWs5Hv0O4D3A4eTPN61fRR4X5JLmR+WOQr8DkBVPZXkLuBp5mfa3OJMGUlaX8uGe1U9DGSRVfe/yj6fBD65irokSavgFaqS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQsuGeZFuSB5M8neSpJB/q2t+U5IEkf9X9fGPXniSfSXIkyRNJLht0JyRJr7SSM/dTwK6qegtwJXBLkrcAu4EDVXUxcKBbBngXcHH32AncseZVS5Je1bLhXlXPVdVj3fOfAM8AW4FrgX3dZvuA67rn1wJfqHmPAJuTbFnrwiVJS0tVrXzjZDvwEHAJ8L+ranPXHuBEVW1Oci9wW1U93K07ANxaVQfPONZO5s/sGR8fv3x6erqvDszNzTE2NtbXvhvFKPXh8OzJRdvHN8GxFwf3uju2nje4g3dG6X1YzKjXD/ahV1NTU4eqamKxdWev9CBJxoCvAB+uqhfm83xeVVWSlf8rMb/PHmAPwMTERE1OTvay+9+amZmh3303ilHqw02771u0fdeOU9x+eMUfp54dvWFyYMc+bZTeh8WMev1gH9bSimbLJDmH+WD/UlV9tWs+dnq4pft5vGufBbYt2P3Crk2StE5WMlsmwF7gmar61IJV+4Ebu+c3Al9b0P6BbtbMlcDJqnpuDWuWJC1jJd+j3wG8Hzic5PGu7aPAbcBdSW4Gvg+8t1t3P/Bu4AjwM+CDa1mwJGl5y4Z794fRLLH66kW2L+CWVdYlSVoFr1CVpAYZ7pLUIMNdkhpkuEtSgwx3SWrQ4C4plBqxfYmrcgft6G3vGcrrqg2euUtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa5EVMGgnrcSHRrh2nlvxvBKVR45m7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatCy4Z7k80mOJ3lyQdvHk8wmebx7vHvBuo8kOZLkO0l+fVCFS5KWtpIz9zuBaxZp/3RVXdo97gdI8hbgeuCXu33+c5Kz1qpYSdLKLBvuVfUQ8OMVHu9aYLqqXqqq7wFHgCtWUZ8kqQ+pquU3SrYD91bVJd3yx4GbgBeAg8CuqjqR5A+BR6rqi912e4GvV9XdixxzJ7ATYHx8/PLp6em+OjA3N8fY2Fhf+24Uo9SHw7MnF20f3wTHXlznYtbYRuvDjq3n9bT9KH2OlmIfejM1NXWoqiYWW9fv7QfuAP4AqO7n7cBv93KAqtoD7AGYmJioycnJvgqZmZmh3303ilHqw1KX5+/acYrbD4/23Sw2Wh+O3jDZ0/aj9Dlain1YO33NlqmqY1X1clX9HPgj/m7oZRbYtmDTC7s2SdI66ivck2xZsPibwOmZNPuB65O8LslFwMXAo6srUZLUq2W/gyb5MjAJnJ/kWeBjwGSSS5kfljkK/A5AVT2V5C7gaeAUcEtVvTyQyiVJS1o23KvqfYs0732V7T8JfHI1RUmSVscrVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KBlwz3J55McT/LkgrY3JXkgyV91P9/YtSfJZ5IcSfJEkssGWbwkaXErOXO/E7jmjLbdwIGquhg40C0DvAu4uHvsBO5YmzIlSb1YNtyr6iHgx2c0Xwvs657vA65b0P6FmvcIsDnJljWqVZK0Qv2OuY9X1XPd8x8C493zrcAPFmz3bNcmSVpHqarlN0q2A/dW1SXd8vNVtXnB+hNV9cYk9wK3VdXDXfsB4NaqOrjIMXcyP3TD+Pj45dPT0311YG5ujrGxsb723ShGqQ+HZ08u2j6+CY69uM7FrLGN1ocdW8/raftR+hwtxT70Zmpq6lBVTSy27uw+j3ksyZaqeq4bdjnetc8C2xZsd2HX9guqag+wB2BiYqImJyf7KmRmZoZ+990oRqkPN+2+b9H2XTtOcfvhfj9OG8NG68PRGyZ72n6UPkdLsQ9rp99hmf3Ajd3zG4GvLWj/QDdr5krg5ILhG0nSOln2NCXJl4FJ4PwkzwIfA24D7kpyM/B94L3d5vcD7waOAD8DPjiAmiVJy1g23KvqfUusunqRbQu4ZbVFSZJWxytUJalBhrskNchwl6QGGe6S1KCNM6lX0itsX+KagqXs2nFqyesQenH0tves+hgaPs/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDvOWvpFfo9VbDa+nOa84d2mu3xjN3SWrQqs7ckxwFfgK8DJyqqokkbwL+DNgOHAXeW1UnVlemJKkXa3HmPlVVl1bVRLe8GzhQVRcDB7plSdI6GsSwzLXAvu75PuC6AbyGJOlVpKr63zn5HnACKOC/VNWeJM9X1eZufYATp5fP2HcnsBNgfHz88unp6b5qmJubY2xsrL8ObBCj1IfDsycXbR/fBMdeXOdi1tio92HU6we46LyzRuZ3YSnr+fs8NTV1aMGoySusNty3VtVskn8APAD8HrB/YZgnOVFVb3y140xMTNTBgwf7qmFmZobJycm+9t0oRqkPS82k2LXjFLcfHu3JV6Peh1GvH+Zny4zK78JS1vP3OcmS4b6qYZmqmu1+HgfuAa4AjiXZ0r3wFuD4al5DktS7vsM9yblJ3nD6OfBrwJPAfuDGbrMbga+ttkhJUm9W8x1uHLhnflids4E/rao/T/JN4K4kNwPfB967+jIlSb3oO9yr6rvAryzS/tfA1aspSpK0Ol6hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg0b7RhSSmnJ49iQ3DeF/gjp623vW/TUHzTN3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNPLz3Ic1LxbanBsrqQ2euUtSgwx3SWqQ4S5JDTLcJalBhrskNWhg4Z7kmiTfSXIkye5BvY4k6RcNZCpkkrOAzwLvBJ4Fvplkf1U9PYjXk6TV2L6G06l37TjV0/TsQU2pHtQ89yuAI1X1XYAk08C1gOG+BtbygyipTamqtT9o8lvANVX1L7vl9wNvq6rfXbDNTmBnt/hLwHf6fLnzgR+totyNwD5sDKPeh1GvH+xDr/5RVV2w2IqhXaFaVXuAPas9TpKDVTWxBiUNjX3YGEa9D6NeP9iHtTSoP6jOAtsWLF/YtUmS1sGgwv2bwMVJLkryWuB6YP+AXkuSdIaBDMtU1akkvwv8V+As4PNV9dQgXos1GNrZAOzDxjDqfRj1+sE+rJmB/EFVkjRcXqEqSQ0y3CWpQSMd7qN+i4Mkn09yPMmTw66lX0m2JXkwydNJnkryoWHX1Iskr0/yaJJvdfV/Ytg19SvJWUn+Msm9w66lH0mOJjmc5PEkB4ddTz+SbE5yd5JvJ3kmyduHVsuojrl3tzj4nyy4xQHwvlG6xUGSXwXmgC9U1SXDrqcfSbYAW6rqsSRvAA4B143K+5AkwLlVNZfkHOBh4ENV9ciQS+tZkt8HJoC/X1W/Mex6epXkKDBRVSN7EVOSfcD/qKrPdTMF/15VPT+MWkb5zP1vb3FQVX8DnL7FwcioqoeAHw+7jtWoqueq6rHu+U+AZ4Ctw61q5WreXLd4TvcYuTOeJBcC7wE+N+xa/n+V5DzgV4G9AFX1N8MKdhjtcN8K/GDB8rOMUKi0KMl24K3AXwy5lJ50wxmPA8eBB6pqpOrv/Cfg3wA/H3Idq1HAf0tyqLs9yai5CPg/wB93w2OfS3LusIoZ5XDXBpJkDPgK8OGqemHY9fSiql6uqkuZv5L6iiQjNUSW5DeA41V1aNi1rNI/rarLgHcBt3TDlqPkbOAy4I6qeivwU2Bofwsc5XD3FgcbRDdW/RXgS1X11WHX06/uK/SDwDVDLqVX7wD+WTdmPQ1cleSLwy2pd1U12/08DtzD/NDrKHkWeHbBN7+7mQ/7oRjlcPcWBxtA9wfJvcAzVfWpYdfTqyQXJNncPd/E/B/ovz3UonpUVR+pqgurajvzvwffqKp/MeSyepLk3O4P8nRDGb8GjNQssqr6IfCDJL/UNV3NEG9zPrS7Qq7WOt/iYCCSfBmYBM5P8izwsaraO9yqevYO4P3A4W7cGuCjVXX/8ErqyRZgXzf76jXAXVU1klMJR9w4cM/8uQJnA39aVX8+3JL68nvAl7oTzu8CHxxWISM7FVKStLRRHpaRJC3BcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN+n91EGMVQb34awAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"It looks from the `describe()` output like `Pclass` contains just 3 values, which we can confirm by looking at the [Data Dictionary](https://www.kaggle.com/competitions/titanic/data) (which you should always study carefully for any project!) -- ","metadata":{"hidden":true}},{"cell_type":"code","source":"pclasses = sorted(training_dataframe.Pclass.unique())\npclasses","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.606464Z","iopub.execute_input":"2022-09-07T20:32:32.606901Z","iopub.status.idle":"2022-09-07T20:32:32.613285Z","shell.execute_reply.started":"2022-09-07T20:32:32.606863Z","shell.execute_reply":"2022-09-07T20:32:32.612583Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[1, 2, 3]"},"metadata":{}}]},{"cell_type":"markdown","source":"Here's how we get a quick summary of all the non-numeric columns in the dataset:","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe.describe(include=[object])","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.617174Z","iopub.execute_input":"2022-09-07T20:32:32.617861Z","iopub.status.idle":"2022-09-07T20:32:32.639151Z","shell.execute_reply.started":"2022-09-07T20:32:32.617819Z","shell.execute_reply":"2022-09-07T20:32:32.638369Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                           Name   Sex  Ticket    Cabin Embarked\ncount                       891   891     891      891      891\nunique                      891     2     681      147        3\ntop     Braund, Mr. Owen Harris  male  347082  B96 B98        S\nfreq                          1   577       7      691      646","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Ticket</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891</td>\n      <td>891</td>\n      <td>891</td>\n      <td>891</td>\n      <td>891</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>891</td>\n      <td>2</td>\n      <td>681</td>\n      <td>147</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>347082</td>\n      <td>B96 B98</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>577</td>\n      <td>7</td>\n      <td>691</td>\n      <td>646</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Clearly we can't multiply strings like `male` or `S` by coefficients, so we need to replace those with numbers.\n\nWe do that by creating new columns containing *dummy variables*. A dummy variable is a column that contains a `1` where a particular column contains a particular value, or a `0` otherwise. For instance, we could create a dummy variable for `Sex='male'`, which would be a new column containing `1` for rows where `Sex` is `'male'`, and 0 for rows where it isn't.\n\nPandas can create these automatically using `get_dummies`, which also remove the original columns. We'll create dummy variables for `Pclass`, even though it's numeric, since the numbers `1`, `2`, and `3` correspond to first, second, and third class cabins - not to counts or measures that make sense to multiply by. We'll also create dummies for `Sex` and `Embarked` since we'll want to use those as predictors in our model. On the other hand, `Cabin`, `Name`, and `Ticket` have too many unique values for it to make sense creating dummy variables for them.","metadata":{"hidden":true}},{"cell_type":"code","source":"training_dataframe = pd.get_dummies(training_dataframe, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ntraining_dataframe.columns","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.640434Z","iopub.execute_input":"2022-09-07T20:32:32.640869Z","iopub.status.idle":"2022-09-07T20:32:32.652767Z","shell.execute_reply.started":"2022-09-07T20:32:32.640833Z","shell.execute_reply":"2022-09-07T20:32:32.651878Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Index(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that 5 columns have been added to the end -- one for each of the possible values of each of the three columns we requested, and that those three requested columns have been removed.\n\nHere's what the first few rows of those newly added columns look like:","metadata":{"hidden":true}},{"cell_type":"code","source":"added_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ntraining_dataframe[added_cols].head()","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.654145Z","iopub.execute_input":"2022-09-07T20:32:32.654684Z","iopub.status.idle":"2022-09-07T20:32:32.667959Z","shell.execute_reply.started":"2022-09-07T20:32:32.654648Z","shell.execute_reply":"2022-09-07T20:32:32.667205Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"   Sex_male  Sex_female  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n0         1           0         0         0         1           0           0           1\n1         0           1         1         0         0           1           0           0\n2         0           1         0         0         1           0           0           1\n3         0           1         1         0         0           0           0           1\n4         1           0         0         0         1           0           0           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex_male</th>\n      <th>Sex_female</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is `Survived`:","metadata":{"hidden":true}},{"cell_type":"code","source":"from torch import tensor\n\nt_dep = tensor(training_dataframe.Survived)","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.669549Z","iopub.execute_input":"2022-09-07T20:32:32.669847Z","iopub.status.idle":"2022-09-07T20:32:32.678763Z","shell.execute_reply.started":"2022-09-07T20:32:32.669812Z","shell.execute_reply":"2022-09-07T20:32:32.677988Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Our independent variables are all the continuous variables of interest plus all the dummy variables we just created:","metadata":{"hidden":true}},{"cell_type":"code","source":"indep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\n# Grab the independent columns we want to use and make a tensor out of them.\n# Pytorch wants everything to be floats for when it multiplies things together, so we're making everything floats.\nt_indep = tensor(training_dataframe[indep_cols].values, dtype=torch.float)\nt_indep","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.681432Z","iopub.execute_input":"2022-09-07T20:32:32.681662Z","iopub.status.idle":"2022-09-07T20:32:32.695842Z","shell.execute_reply.started":"2022-09-07T20:32:32.681629Z","shell.execute_reply":"2022-09-07T20:32:32.695014Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        ...,\n        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])"},"metadata":{}}]},{"cell_type":"markdown","source":"The shape is going to be important because you need things to line up to do matrix multiplication.\nHere's the number of rows and columns we have for our independent variables:","metadata":{"hidden":true}},{"cell_type":"code","source":"t_indep.shape","metadata":{"hidden":true,"execution":{"iopub.status.busy":"2022-09-07T20:32:32.698456Z","iopub.execute_input":"2022-09-07T20:32:32.698667Z","iopub.status.idle":"2022-09-07T20:32:32.706706Z","shell.execute_reply.started":"2022-09-07T20:32:32.698642Z","shell.execute_reply":"2022-09-07T20:32:32.705921Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([891, 12])"},"metadata":{}}]},{"cell_type":"markdown","source":"The rank is the number of dimenstions. You can see that by getting the length of the shape.","metadata":{}},{"cell_type":"code","source":"len(t_indep.shape)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.709282Z","iopub.execute_input":"2022-09-07T20:32:32.709789Z","iopub.status.idle":"2022-09-07T20:32:32.714945Z","shell.execute_reply.started":"2022-09-07T20:32:32.709760Z","shell.execute_reply":"2022-09-07T20:32:32.714159Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"markdown","source":"- Scalar is Rank 0 (just one number, x=tensor(2.0) is a scalar)\n- Vector is Rank 1\n- Matrix is Rank 2\n- Others are just referred to by rank","metadata":{}},{"cell_type":"markdown","source":"## Setting up a linear model","metadata":{}},{"cell_type":"markdown","source":"Now that we've got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we're going to manually do a single step of calculating predictions and loss for every row of our data.\n\nOur first model will be a simple linear model. We'll need a coefficient for each column in `t_indep`. We'll pick random numbers in the range `(-0.5,0.5)`, and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(442)\n\n# number of coefficients is equal to the number of columns. We get the number of columns from t_indep.shape[1] here\nn_coeff = t_indep.shape[1]\n# torch.rand() generates numbers between 0 and 1, if we subtract 0.5 from all of them, they'll be centered on 0\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.716401Z","iopub.execute_input":"2022-09-07T20:32:32.716883Z","iopub.status.idle":"2022-09-07T20:32:32.727303Z","shell.execute_reply.started":"2022-09-07T20:32:32.716848Z","shell.execute_reply":"2022-09-07T20:32:32.726440Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])"},"metadata":{}}]},{"cell_type":"markdown","source":"Our predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don't need a separate constant term (also known as a \"bias\" or \"intercept\" term), or a column of all `1`s to give the same effect has having a constant term. That's because our dummy variables already cover the entire dataset -- e.g. there's a column for \"male\" and a column for \"female\", and everyone in the dataset is in exactly one of these; therefore, we don't need a separate intercept term to cover rows that aren't otherwise part of a column.\n\nHere's what the multiplication looks like:","metadata":{}},{"cell_type":"code","source":"t_indep.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.729263Z","iopub.execute_input":"2022-09-07T20:32:32.729886Z","iopub.status.idle":"2022-09-07T20:32:32.737408Z","shell.execute_reply.started":"2022-09-07T20:32:32.729843Z","shell.execute_reply":"2022-09-07T20:32:32.735497Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([891, 12])"},"metadata":{}}]},{"cell_type":"code","source":"coeffs.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.739871Z","iopub.execute_input":"2022-09-07T20:32:32.740560Z","iopub.status.idle":"2022-09-07T20:32:32.745951Z","shell.execute_reply.started":"2022-09-07T20:32:32.740521Z","shell.execute_reply":"2022-09-07T20:32:32.745106Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"torch.Size([12])"},"metadata":{}}]},{"cell_type":"markdown","source":"The coeffs values will be multiplied against each of the 891 rows of t_indep, one by one. This works if both variables are tensors\nand if the last values in the shape of each tensor, match, i.e. are the same, then numpy will do what's called a broadcast and make\nthis very efficient. And if you're using a GPU, then it will compile to use teh GPU efficiently.","metadata":{}},{"cell_type":"code","source":"# multiply a matrix by a vector\nt_indep*coeffs","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.747522Z","iopub.execute_input":"2022-09-07T20:32:32.748163Z","iopub.status.idle":"2022-09-07T20:32:32.757148Z","shell.execute_reply.started":"2022-09-07T20:32:32.748124Z","shell.execute_reply":"2022-09-07T20:32:32.756249Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([[-10.1838,   0.1386,   0.0000,  -0.4772,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-17.5902,   0.1386,   0.0000,  -0.9681,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-12.0354,   0.0000,   0.0000,  -0.4950,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.1386,   0.0000,  -0.9025,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.0000,   0.0000,  -0.4982,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.0000,   0.0000,  -0.5081,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-24.9966,   0.0000,   0.0000,  -0.8973,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        ...,\n        [-11.5725,   0.0000,   0.0000,  -0.4717,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-18.0531,   0.0000,   1.2045,  -0.7701,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-12.4983,   0.0000,   0.0000,  -0.5968,  -0.2632,  -0.0000,   0.0000,   0.3136,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [ -8.7951,   0.0000,   0.0000,  -0.7766,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.1386,   0.4818,  -0.7229,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-12.0354,   0.0000,   0.0000,  -0.7766,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-14.8128,   0.0000,   0.0000,  -0.4905,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000]])"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see we've got a problem here. The sums of each row will be dominated by the first column, which is `Age`, since that's bigger on average than all the others.\n\nLet's make all the columns contain numbers from `0` to `1`, by dividing each column by its `max()`, what we're doing is normalizing\nthis column:","metadata":{}},{"cell_type":"code","source":"# Max of each row by calling max(). Since Age is the biggest value as we've already normalized the other values.\n# Passing the dimension is, do you want the maximum of the rows? or the maximum of the columns?\n# In this case since we're just getting the max of the one row, we also want the max over the row, vs the max\n# over the column. Which we're doing when we say dim=0.\n# Other frameworks will refer to dim, dimensions as axes. Pytorch calls them dimensions, dim.\n\n# value of each maximum and the index of where it was. In this case which row it was.\nvals,indices = t_indep.max(dim=0)\n\n# Thanks to broadcasting, we do the following:\nt_indep = t_indep / vals","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.758648Z","iopub.execute_input":"2022-09-07T20:32:32.759395Z","iopub.status.idle":"2022-09-07T20:32:32.765126Z","shell.execute_reply.started":"2022-09-07T20:32:32.759351Z","shell.execute_reply":"2022-09-07T20:32:32.764359Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Common normalizations you see are dividing by the maximum, which we do here. And another is subtracting the mean and dividing by\nthe standard deviation. Doesn't matter too much which you use, usually.","metadata":{}},{"cell_type":"markdown","source":"As we see, that removes the problem of one column dominating all the others:","metadata":{}},{"cell_type":"code","source":"t_indep*coeffs","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.766496Z","iopub.execute_input":"2022-09-07T20:32:32.767017Z","iopub.status.idle":"2022-09-07T20:32:32.779650Z","shell.execute_reply.started":"2022-09-07T20:32:32.766983Z","shell.execute_reply":"2022-09-07T20:32:32.778681Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1504,  0.0000,  0.0000, -0.0793, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0173,  0.0000, -0.1446, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0000,  0.0000, -0.0798, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0000,  0.0000, -0.0814, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.3125,  0.0000,  0.0000, -0.1438, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        ...,\n        [-0.1447,  0.0000,  0.0000, -0.0756, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2257,  0.0000,  0.2008, -0.1234, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.1562,  0.0000,  0.0000, -0.0956, -0.2632, -0.0000,  0.0000,  0.3136,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1099,  0.0000,  0.0000, -0.1244, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0173,  0.0803, -0.1158, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1504,  0.0000,  0.0000, -0.1244, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1852,  0.0000,  0.0000, -0.0786, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000]])"},"metadata":{}}]},{"cell_type":"markdown","source":"One thing you hopefully noticed is how amazingly cool this line of code is:\n\n    t_indep = t_indep / vals\n\nThat is dividing a matrix by a vector -- what on earth does that mean?!? The trick here is that we're taking advantage of a technique in numpy and PyTorch (and many other languages, going all the way back to APL) called [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html). In short, this acts as if there's a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn't actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we're using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it's well worth studying and practicing.\n\nWe can now create predictions from our linear model, by adding up the rows of the product:","metadata":{}},{"cell_type":"code","source":"# Sum up over the columns\npreds = (t_indep*coeffs).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.781178Z","iopub.execute_input":"2022-09-07T20:32:32.781821Z","iopub.status.idle":"2022-09-07T20:32:32.787844Z","shell.execute_reply.started":"2022-09-07T20:32:32.781776Z","shell.execute_reply":"2022-09-07T20:32:32.787044Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the first few:","metadata":{}},{"cell_type":"code","source":"preds[:10]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.788898Z","iopub.execute_input":"2022-09-07T20:32:32.789200Z","iopub.status.idle":"2022-09-07T20:32:32.799314Z","shell.execute_reply.started":"2022-09-07T20:32:32.789166Z","shell.execute_reply":"2022-09-07T20:32:32.798561Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.1927, -0.6239,  0.0979,  0.2056,  0.0968,  0.0066,  0.1306,  0.3476,  0.1613, -0.6285])"},"metadata":{}}]},{"cell_type":"markdown","source":"Of course, these predictions aren't going to be any use, since our coefficients are random -- they're just a starting point for our gradient descent process.\n\nTo do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:","metadata":{}},{"cell_type":"code","source":"# Mean absolute value loss function. Easy one to just get something out there.\nloss = torch.abs(preds-t_dep).mean()\nloss","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.800428Z","iopub.execute_input":"2022-09-07T20:32:32.801114Z","iopub.status.idle":"2022-09-07T20:32:32.810223Z","shell.execute_reply.started":"2022-09-07T20:32:32.801075Z","shell.execute_reply":"2022-09-07T20:32:32.809486Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"tensor(0.5382)"},"metadata":{}}]},{"cell_type":"markdown","source":"Now that we've tested out a way of calculating predictions, and loss, let's pop them into functions to make life easier:","metadata":{}},{"cell_type":"code","source":"def calculate_predictions(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calculate_loss(coeffs, indeps, deps): return torch.abs(calculate_predictions(coeffs, indeps)-deps).mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.811359Z","iopub.execute_input":"2022-09-07T20:32:32.812104Z","iopub.status.idle":"2022-09-07T20:32:32.818900Z","shell.execute_reply.started":"2022-09-07T20:32:32.812069Z","shell.execute_reply":"2022-09-07T20:32:32.818178Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Doing a gradient descent step","metadata":{}},{"cell_type":"markdown","source":"In this section, we're going to do a single \"epoch\" of gradient descent manually. The only thing we're going to automate is calculating gradients, because let's face it that's pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we'll need to call `requires_grad_()` on our `coeffs` (if you're not sure why, review the previous notebook, [How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work), before continuing):","metadata":{}},{"cell_type":"code","source":"# In PyTorch, if there's an '_' at the end, it's an in-place operation and will change the variable directly, versus leave\n# the variable unchanged.\ncoeffs.requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.820038Z","iopub.execute_input":"2022-09-07T20:32:32.820731Z","iopub.status.idle":"2022-09-07T20:32:32.833258Z","shell.execute_reply.started":"2022-09-07T20:32:32.820694Z","shell.execute_reply":"2022-09-07T20:32:32.832441Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"Now when we calculate our loss, PyTorch will keep track of all the steps, so we'll be able to get the gradients afterwards:","metadata":{}},{"cell_type":"code","source":"loss = calculate_loss(coeffs, t_indep, t_dep)\nloss","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.834960Z","iopub.execute_input":"2022-09-07T20:32:32.837243Z","iopub.status.idle":"2022-09-07T20:32:32.843468Z","shell.execute_reply.started":"2022-09-07T20:32:32.837214Z","shell.execute_reply":"2022-09-07T20:32:32.842778Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor(0.5382, grad_fn=<MeanBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"With requires_grad=True, Python knows now what it needs to do to go backward from the loss values to get the gradient values.\nUse `backward()` to ask PyTorch to calculate gradients now, from the loss values:","metadata":{}},{"cell_type":"code","source":"loss.backward()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.844979Z","iopub.execute_input":"2022-09-07T20:32:32.845371Z","iopub.status.idle":"2022-09-07T20:32:32.876162Z","shell.execute_reply.started":"2022-09-07T20:32:32.845336Z","shell.execute_reply":"2022-09-07T20:32:32.875303Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"backward() will put the values into a variable that exists on the coeffs variable. You can access it with `coeffs.grad`\nLet's see what they look like:","metadata":{}},{"cell_type":"code","source":"# The coefficients' gradients\ncoeffs.grad","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.877739Z","iopub.execute_input":"2022-09-07T20:32:32.877998Z","iopub.status.idle":"2022-09-07T20:32:32.884501Z","shell.execute_reply.started":"2022-09-07T20:32:32.877963Z","shell.execute_reply":"2022-09-07T20:32:32.883658Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"tensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])"},"metadata":{}}]},{"cell_type":"markdown","source":"Note that each time we call `backward`, the gradients are actually *added* to whatever is in the `.grad` attribute. Let's try running the above steps again:","metadata":{}},{"cell_type":"code","source":"loss = calculate_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.886288Z","iopub.execute_input":"2022-09-07T20:32:32.886866Z","iopub.status.idle":"2022-09-07T20:32:32.896002Z","shell.execute_reply.started":"2022-09-07T20:32:32.886828Z","shell.execute_reply":"2022-09-07T20:32:32.895008Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"tensor([-0.0212,  0.0258, -0.0082, -0.0969,  0.4198, -0.4265, -0.2424, -0.0494,  0.2851, -0.3771, -0.0382,  0.4085])"},"metadata":{}}]},{"cell_type":"markdown","source":"As you see, our `.grad` values are have doubled. That's because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\n\nWe can now do one gradient descent step, and check that our loss decreases:","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1) # 0.1 is the learning rate\n    coeffs.grad.zero_()\n    print(calculate_loss(coeffs, t_indep, t_dep))","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.897863Z","iopub.execute_input":"2022-09-07T20:32:32.898251Z","iopub.status.idle":"2022-09-07T20:32:32.906647Z","shell.execute_reply.started":"2022-09-07T20:32:32.898190Z","shell.execute_reply":"2022-09-07T20:32:32.905412Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"tensor(0.5043)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note that `a.sub_(b)` subtracts `b` from `a` in-place. In PyTorch, any method that ends in `_` changes its object in-place. Similarly, `a.zero_()` sets all elements of a tensor to zero.","metadata":{}},{"cell_type":"markdown","source":"## Training the linear model","metadata":{}},{"cell_type":"markdown","source":"Before we begin training our model, we'll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see \"[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners#Test-and-validation-sets)\".\n\nThere's lots of different ways we can do this. In the next notebook we'll be comparing our approach here to what the fastai library does, so we'll want to ensure we split the data in the same way. So let's use `RandomSplitter` to get indices that will split our data into training and validation sets:","metadata":{}},{"cell_type":"code","source":"from fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(training_dataframe)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:32.907987Z","iopub.execute_input":"2022-09-07T20:32:32.908281Z","iopub.status.idle":"2022-09-07T20:32:33.465509Z","shell.execute_reply.started":"2022-09-07T20:32:32.908242Z","shell.execute_reply":"2022-09-07T20:32:33.464739Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Now we can apply those indicies to our independent and dependent variables:","metadata":{}},{"cell_type":"code","source":"trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.466706Z","iopub.execute_input":"2022-09-07T20:32:33.467215Z","iopub.status.idle":"2022-09-07T20:32:33.478971Z","shell.execute_reply.started":"2022-09-07T20:32:33.467168Z","shell.execute_reply":"2022-09-07T20:32:33.477875Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(713, 178)"},"metadata":{}}]},{"cell_type":"markdown","source":"We'll create functions for the three things we did manually above: updating `coeffs`, doing one full gradient descent step, and initilizing `coeffs` to random numbers:","metadata":{}},{"cell_type":"code","source":"def update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.480929Z","iopub.execute_input":"2022-09-07T20:32:33.481308Z","iopub.status.idle":"2022-09-07T20:32:33.486359Z","shell.execute_reply.started":"2022-09-07T20:32:33.481270Z","shell.execute_reply":"2022-09-07T20:32:33.485053Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def one_epoch(coeffs, lr):\n    loss = calculate_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.487966Z","iopub.execute_input":"2022-09-07T20:32:33.488900Z","iopub.status.idle":"2022-09-07T20:32:33.495735Z","shell.execute_reply.started":"2022-09-07T20:32:33.488854Z","shell.execute_reply":"2022-09-07T20:32:33.495007Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.502093Z","iopub.execute_input":"2022-09-07T20:32:33.502815Z","iopub.status.idle":"2022-09-07T20:32:33.508871Z","shell.execute_reply.started":"2022-09-07T20:32:33.502775Z","shell.execute_reply":"2022-09-07T20:32:33.508077Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"We can now use these functions to train our model:","metadata":{}},{"cell_type":"code","source":"def train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.510050Z","iopub.execute_input":"2022-09-07T20:32:33.510400Z","iopub.status.idle":"2022-09-07T20:32:33.518967Z","shell.execute_reply.started":"2022-09-07T20:32:33.510364Z","shell.execute_reply":"2022-09-07T20:32:33.518116Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Let's try it. Our loss will print at the end of every step, so we hope we'll see it going down:","metadata":{}},{"cell_type":"code","source":"coeffs = train_model(18, lr=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.520410Z","iopub.execute_input":"2022-09-07T20:32:33.520689Z","iopub.status.idle":"2022-09-07T20:32:33.539553Z","shell.execute_reply.started":"2022-09-07T20:32:33.520651Z","shell.execute_reply":"2022-09-07T20:32:33.538473Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; ","output_type":"stream"}]},{"cell_type":"markdown","source":"It does! This is a working linear model.\n\nLet's take a look at the coefficients for each column:","metadata":{}},{"cell_type":"code","source":"def show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.540783Z","iopub.execute_input":"2022-09-07T20:32:33.541103Z","iopub.status.idle":"2022-09-07T20:32:33.552939Z","shell.execute_reply.started":"2022-09-07T20:32:33.541068Z","shell.execute_reply":"2022-09-07T20:32:33.552070Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'Age': tensor(-0.2694),\n 'SibSp': tensor(0.0901),\n 'Parch': tensor(0.2359),\n 'LogFare': tensor(0.0280),\n 'Sex_male': tensor(-0.3990),\n 'Sex_female': tensor(0.2345),\n 'Pclass_1': tensor(0.7232),\n 'Pclass_2': tensor(0.4112),\n 'Pclass_3': tensor(0.3601),\n 'Embarked_C': tensor(0.0955),\n 'Embarked_Q': tensor(0.2395),\n 'Embarked_S': tensor(0.2122)}"},"metadata":{}}]},{"cell_type":"markdown","source":"Good to eyeball this output to make sure it makes sense:\n- Older people had less chance of surviving\n- Males had less chance of surviving\n- Pclass 1 had a high chance of surviving","metadata":{}},{"cell_type":"markdown","source":"## Measuring accuracy","metadata":{}},{"cell_type":"markdown","source":"The Kaggle competition is not, however, scored by absolute error (which is our loss function). It's scored by *accuracy* -- the proportion of rows where we correctly predict survival. Let's see how accurate we were on the validation set. First, calculate the predictions:","metadata":{}},{"cell_type":"code","source":"preds = calculate_predictions(coeffs, val_indep)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.554436Z","iopub.execute_input":"2022-09-07T20:32:33.554934Z","iopub.status.idle":"2022-09-07T20:32:33.558942Z","shell.execute_reply.started":"2022-09-07T20:32:33.554896Z","shell.execute_reply":"2022-09-07T20:32:33.558157Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"We'll assume that any passenger with a score of over `0.5` is predicted to survive. So that means we're correct for each row where `preds>0.5` is the same as the dependent variable:","metadata":{}},{"cell_type":"code","source":"#         actual values     Grabbing our results over 0.5, saying that >= 0.5 we'll call as means we predicted survival.\n#                           This is arbitrary and we could say >= 0.8 or other.\nresults = val_dep.bool()==(preds>0.5)\nresults[:16]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.560551Z","iopub.execute_input":"2022-09-07T20:32:33.561088Z","iopub.status.idle":"2022-09-07T20:32:33.571327Z","shell.execute_reply.started":"2022-09-07T20:32:33.561047Z","shell.execute_reply":"2022-09-07T20:32:33.570106Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False])"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's see what our average accuracy is:","metadata":{}},{"cell_type":"code","source":"results.float().mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.573149Z","iopub.execute_input":"2022-09-07T20:32:33.573701Z","iopub.status.idle":"2022-09-07T20:32:33.581422Z","shell.execute_reply.started":"2022-09-07T20:32:33.573665Z","shell.execute_reply":"2022-09-07T20:32:33.580639Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"tensor(0.7865)"},"metadata":{}}]},{"cell_type":"markdown","source":"tensor(0.7865) means we were right 78.65% of the time.\n\nThat's not a bad start at all! We'll create a function so we can calcuate the accuracy easy for other models we train:","metadata":{}},{"cell_type":"code","source":"def acc(coeffs): return (val_dep.bool()==(calculate_predictions(coeffs, val_indep)>0.5)).float().mean()\nacc(coeffs)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.582824Z","iopub.execute_input":"2022-09-07T20:32:33.583393Z","iopub.status.idle":"2022-09-07T20:32:33.591479Z","shell.execute_reply.started":"2022-09-07T20:32:33.583355Z","shell.execute_reply":"2022-09-07T20:32:33.590500Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor(0.7865)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Using sigmoid","metadata":{}},{"cell_type":"markdown","source":"Looking at our predictions, there's one obvious problem -- some of our predictions of the probability of survival are `>1`, and some are `<0`:","metadata":{}},{"cell_type":"code","source":"preds[:28]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.592731Z","iopub.execute_input":"2022-09-07T20:32:33.593428Z","iopub.status.idle":"2022-09-07T20:32:33.601828Z","shell.execute_reply.started":"2022-09-07T20:32:33.593337Z","shell.execute_reply":"2022-09-07T20:32:33.600985Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.8160,  0.1295, -0.0148,  0.1831,  0.1520,  0.1350,  0.7279,  0.7754,  0.3222,  0.6740,  0.0753,  0.0389,  0.2216,  0.7631,\n         0.0678,  0.3997,  0.3324,  0.8278,  0.1078,  0.7126,  0.1023,  0.3627,  0.9937,  0.8050,  0.1153,  0.1455,  0.8652,  0.3425])"},"metadata":{}}]},{"cell_type":"markdown","source":"To fix this, we should pass every prediction through the *sigmoid function*, which has a minimum at zero and maximum at one, and is defined as follows:","metadata":{}},{"cell_type":"code","source":"import sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:33.603821Z","iopub.execute_input":"2022-09-07T20:32:33.604958Z","iopub.status.idle":"2022-09-07T20:32:34.186349Z","shell.execute_reply.started":"2022-09-07T20:32:33.604923Z","shell.execute_reply":"2022-09-07T20:32:34.185519Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"PyTorch already defines that function for us, so we can modify `calculate_predictions` to use it:","metadata":{}},{"cell_type":"code","source":"def calculate_predictions(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.187590Z","iopub.execute_input":"2022-09-07T20:32:34.188327Z","iopub.status.idle":"2022-09-07T20:32:34.193002Z","shell.execute_reply.started":"2022-09-07T20:32:34.188288Z","shell.execute_reply":"2022-09-07T20:32:34.192030Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Let's train a new model now, using this updated function to calculate predictions:","metadata":{}},{"cell_type":"code","source":"coeffs = train_model(lr=100)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.194255Z","iopub.execute_input":"2022-09-07T20:32:34.194989Z","iopub.status.idle":"2022-09-07T20:32:34.222049Z","shell.execute_reply.started":"2022-09-07T20:32:34.194953Z","shell.execute_reply":"2022-09-07T20:32:34.221325Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; ","output_type":"stream"}]},{"cell_type":"markdown","source":"The loss has improved by a lot. Let's check the accuracy:","metadata":{}},{"cell_type":"code","source":"acc(coeffs)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.224413Z","iopub.execute_input":"2022-09-07T20:32:34.224802Z","iopub.status.idle":"2022-09-07T20:32:34.231275Z","shell.execute_reply.started":"2022-09-07T20:32:34.224773Z","shell.execute_reply":"2022-09-07T20:32:34.230540Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"tensor(0.8258)"},"metadata":{}}]},{"cell_type":"markdown","source":"That's improved too! Here's the coefficients of our trained model:","metadata":{}},{"cell_type":"code","source":"show_coeffs()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.232731Z","iopub.execute_input":"2022-09-07T20:32:34.233137Z","iopub.status.idle":"2022-09-07T20:32:34.243910Z","shell.execute_reply.started":"2022-09-07T20:32:34.233102Z","shell.execute_reply":"2022-09-07T20:32:34.243107Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'Age': tensor(-1.5061),\n 'SibSp': tensor(-1.1575),\n 'Parch': tensor(-0.4267),\n 'LogFare': tensor(0.2543),\n 'Sex_male': tensor(-10.3320),\n 'Sex_female': tensor(8.4185),\n 'Pclass_1': tensor(3.8389),\n 'Pclass_2': tensor(2.1398),\n 'Pclass_3': tensor(-6.2331),\n 'Embarked_C': tensor(1.4771),\n 'Embarked_Q': tensor(2.1168),\n 'Embarked_S': tensor(-4.7958)}"},"metadata":{}}]},{"cell_type":"markdown","source":"These coefficients seem reasonable -- in general, older people and males were less likely to survive, and first class passengers were more likely to survive.","metadata":{}},{"cell_type":"markdown","source":"## Submitting to Kaggle","metadata":{}},{"cell_type":"markdown","source":"Now that we've got a trained model, we can prepare a submission to Kaggle. To do that, first we need to read the test set:","metadata":{}},{"cell_type":"code","source":"tst_df = pd.read_csv(path/'test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.246030Z","iopub.execute_input":"2022-09-07T20:32:34.247749Z","iopub.status.idle":"2022-09-07T20:32:34.256493Z","shell.execute_reply.started":"2022-09-07T20:32:34.247712Z","shell.execute_reply":"2022-09-07T20:32:34.255733Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"In this case, it turns out that the test set is missing `Fare` for one passenger. We'll just fill it with `0` to avoid problems:","metadata":{}},{"cell_type":"code","source":"tst_df['Fare'] = tst_df.Fare.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.259070Z","iopub.execute_input":"2022-09-07T20:32:34.259904Z","iopub.status.idle":"2022-09-07T20:32:34.264638Z","shell.execute_reply.started":"2022-09-07T20:32:34.259865Z","shell.execute_reply":"2022-09-07T20:32:34.263888Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Now we can just copy the same steps we did to our training set and do the same exact things on our test set to preprocess the data:","metadata":{}},{"cell_type":"code","source":"tst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.266173Z","iopub.execute_input":"2022-09-07T20:32:34.266451Z","iopub.status.idle":"2022-09-07T20:32:34.286298Z","shell.execute_reply.started":"2022-09-07T20:32:34.266416Z","shell.execute_reply":"2022-09-07T20:32:34.285556Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Let's calculate our predictions of which passengers survived in the test set:","metadata":{}},{"cell_type":"code","source":"tst_df['Survived'] = (calculate_predictions(tst_indep, coeffs)>0.5).int()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.287461Z","iopub.execute_input":"2022-09-07T20:32:34.287869Z","iopub.status.idle":"2022-09-07T20:32:34.293052Z","shell.execute_reply.started":"2022-09-07T20:32:34.287834Z","shell.execute_reply":"2022-09-07T20:32:34.292210Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"The sample submission on the Kaggle competition site shows that we're expected to upload a CSV with just `PassengerId` and `Survived`, so let's create that and save it:","metadata":{}},{"cell_type":"code","source":"sub_df = tst_df[['PassengerId','Survived']]\nif on_kaggle: sub_df.to_csv('submission.csv', index=False)\nelse: sub_df.to_csv(path/'submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.294276Z","iopub.execute_input":"2022-09-07T20:32:34.294766Z","iopub.status.idle":"2022-09-07T20:32:34.305068Z","shell.execute_reply.started":"2022-09-07T20:32:34.294730Z","shell.execute_reply":"2022-09-07T20:32:34.304164Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"We can check the first few rows of the file to make sure it looks reasonable:","metadata":{}},{"cell_type":"code","source":"if on_kaggle:\n    !head submission.csv\nelse:\n    !head data/submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:34.307386Z","iopub.execute_input":"2022-09-07T20:32:34.308439Z","iopub.status.idle":"2022-09-07T20:32:35.284814Z","shell.execute_reply.started":"2022-09-07T20:32:34.308255Z","shell.execute_reply":"2022-09-07T20:32:35.283522Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"PassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,1\n899,0\n900,1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"When you click \"save version\" in Kaggle, and wait for the notebook to run, you'll see that `submission.csv` appears in the \"Data\" tab. Clicking on that file will show a *Submit* button, which allows you to submit to the competition.","metadata":{}},{"cell_type":"markdown","source":"## Using matrix product","metadata":{}},{"cell_type":"markdown","source":"We can make things quite a bit neater...\n\nTake a look at the inner-most calculation we're doing to get the predictions:","metadata":{}},{"cell_type":"code","source":"(val_indep*coeffs).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.286691Z","iopub.execute_input":"2022-09-07T20:32:35.287280Z","iopub.status.idle":"2022-09-07T20:32:35.298002Z","shell.execute_reply.started":"2022-09-07T20:32:35.287239Z","shell.execute_reply":"2022-09-07T20:32:35.297205Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3512, -13.6469,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])"},"metadata":{}}]},{"cell_type":"markdown","source":"Multiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the `@` operator to indicate matrix products, and is supported by PyTorch tensors. Python didn't have an official implementation for\nthe `@` operator, just a definition; since the variables here are tensors, it will use PyTorch's implementation. Therefore,\nwe can replicate the above calculate more simply like so:","metadata":{}},{"cell_type":"code","source":"val_indep@coeffs","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.299721Z","iopub.execute_input":"2022-09-07T20:32:35.300491Z","iopub.status.idle":"2022-09-07T20:32:35.311446Z","shell.execute_reply.started":"2022-09-07T20:32:35.300454Z","shell.execute_reply":"2022-09-07T20:32:35.310764Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310])"},"metadata":{}}]},{"cell_type":"markdown","source":"It also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\n\nLet's use this to replace how `calc_preds` works:","metadata":{}},{"cell_type":"code","source":"def calculate_predictions(coeffs, indeps): return torch.sigmoid(indeps@coeffs)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.312722Z","iopub.execute_input":"2022-09-07T20:32:35.313454Z","iopub.status.idle":"2022-09-07T20:32:35.317827Z","shell.execute_reply.started":"2022-09-07T20:32:35.313419Z","shell.execute_reply":"2022-09-07T20:32:35.317024Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"In order to do matrix-matrix products (which we'll need in the next section), we need to turn `coeffs` into a column vector. This allows us to do matrix multiplication. We can also say a matrix with a single column. \n\nBefore it was an array of one row [1,2,3,4,5] we need to change it to look like this:\n```\n# From one row with n columns to n rows with one column: [1,2,3,4,5,...]\n[\n    1,\n    2,\n    3,\n    4,\n    5,\n    ...\n]\n```\n\nWhich we can do by passing a second argument `1` to `torch.rand()`, indicating that we want our coefficients to have one column:","metadata":{}},{"cell_type":"code","source":"def init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.319004Z","iopub.execute_input":"2022-09-07T20:32:35.319994Z","iopub.status.idle":"2022-09-07T20:32:35.330631Z","shell.execute_reply.started":"2022-09-07T20:32:35.319956Z","shell.execute_reply":"2022-09-07T20:32:35.329766Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"We'll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value `None`, which tells PyTorch to add a new dimension in this position. Effectively this does the above where it goes from an array of one row with n columns, to n rows with one column. This is also called transposing the matrix in math.","metadata":{}},{"cell_type":"code","source":"trn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.331869Z","iopub.execute_input":"2022-09-07T20:32:35.332887Z","iopub.status.idle":"2022-09-07T20:32:35.340424Z","shell.execute_reply.started":"2022-09-07T20:32:35.332847Z","shell.execute_reply":"2022-09-07T20:32:35.339642Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"trn_dep.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.343181Z","iopub.execute_input":"2022-09-07T20:32:35.343391Z","iopub.status.idle":"2022-09-07T20:32:35.351328Z","shell.execute_reply.started":"2022-09-07T20:32:35.343366Z","shell.execute_reply":"2022-09-07T20:32:35.350517Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"torch.Size([713, 1])"},"metadata":{}}]},{"cell_type":"markdown","source":"The 1 axis, when it's just one, we call a unit axis.","metadata":{}},{"cell_type":"markdown","source":"We can now train our model as before and confirm we get identical outputs...:","metadata":{}},{"cell_type":"code","source":"coeffs = train_model(lr=100)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.353349Z","iopub.execute_input":"2022-09-07T20:32:35.353647Z","iopub.status.idle":"2022-09-07T20:32:35.376118Z","shell.execute_reply.started":"2022-09-07T20:32:35.353572Z","shell.execute_reply":"2022-09-07T20:32:35.375304Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; ","output_type":"stream"}]},{"cell_type":"markdown","source":"...and identical accuracy:","metadata":{}},{"cell_type":"code","source":"acc(coeffs)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.377224Z","iopub.execute_input":"2022-09-07T20:32:35.377470Z","iopub.status.idle":"2022-09-07T20:32:35.387297Z","shell.execute_reply.started":"2022-09-07T20:32:35.377436Z","shell.execute_reply":"2022-09-07T20:32:35.386366Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"tensor(0.8258)"},"metadata":{}}]},{"cell_type":"markdown","source":"The coefficients are now a column vector. Also known as, a rank 2 matrix with a trailing unit axis.","metadata":{}},{"cell_type":"code","source":"coeffs","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.388542Z","iopub.execute_input":"2022-09-07T20:32:35.388912Z","iopub.status.idle":"2022-09-07T20:32:35.397270Z","shell.execute_reply.started":"2022-09-07T20:32:35.388849Z","shell.execute_reply":"2022-09-07T20:32:35.396474Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"tensor([[ -1.1308],\n        [ -1.1755],\n        [ -0.6080],\n        [  0.4522],\n        [-10.1322],\n        [  8.6472],\n        [  3.4198],\n        [  1.8521],\n        [ -6.5410],\n        [  1.5472],\n        [  1.9993],\n        [ -4.9103]], requires_grad=True)"},"metadata":{}}]},{"cell_type":"code","source":"coeffs.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.398706Z","iopub.execute_input":"2022-09-07T20:32:35.399719Z","iopub.status.idle":"2022-09-07T20:32:35.405311Z","shell.execute_reply.started":"2022-09-07T20:32:35.399678Z","shell.execute_reply":"2022-09-07T20:32:35.404561Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"torch.Size([12, 1])"},"metadata":{}}]},{"cell_type":"markdown","source":"## A neural network","metadata":{}},{"cell_type":"markdown","source":"We've now got what we need to implement our neural network.\n\nFirst, we'll need to create coefficients for each of our layers. Our first set of coefficients will take our `n_coeff` inputs, and create `n_hidden` outputs. We can choose whatever `n_hidden` we like -- a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size `n_coeff` by `n_hidden`. We'll divide these coefficients by `n_hidden` so that when we sum them up in the next layer we'll end up with similar magnitude numbers to what we started with.\n\nThen our second layer will need to take the `n_hidden` inputs and create a single output, so that means we need a `n_hidden` by `1` matrix there (a column vector). The second layer will also need a constant term added.","metadata":{}},{"cell_type":"code","source":"def init_coeffs(n_hidden=20):\n    #                                       -v- Centralize the values by making them go from -0.5 to 0.5\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    # The coeffs can be too large, since we're summing them up\n    # over 20, or n, hidden layers, it will sum to a large number. We will want to normalize it by\n    # dividing by n_hidden\n    \n    # n_hidden is the numer of column vectors of coefficients that we will use to multiply by in our matrix multiplication\n    \n    # For layer 2 explaination, see video lesson 5, timestamp: 1:05:00, https://www.youtube.com/watch?v=_rXzeWq4C6w&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    # For the 0.3 constant, when done by hand, things are more fidley. This is a contant that just has to be played with\n    # until it starts to get a good result\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.406731Z","iopub.execute_input":"2022-09-07T20:32:35.407900Z","iopub.status.idle":"2022-09-07T20:32:35.414377Z","shell.execute_reply.started":"2022-09-07T20:32:35.407591Z","shell.execute_reply":"2022-09-07T20:32:35.413587Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"Now we have our coefficients, we can create our neural net. The key steps are the two matrix products, `indeps@l1` and `res@l2` (where `res` is the output of the first layer). The first layer output is passed to `F.relu` (that's our non-linearity), and the second is passed to `torch.sigmoid` as before.","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef calculate_predictions(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.415474Z","iopub.execute_input":"2022-09-07T20:32:35.416735Z","iopub.status.idle":"2022-09-07T20:32:35.424846Z","shell.execute_reply.started":"2022-09-07T20:32:35.416690Z","shell.execute_reply":"2022-09-07T20:32:35.423970Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"Finally, now that we have more than one set of coefficients, we need to add a loop to update each one:","metadata":{}},{"cell_type":"code","source":"def update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.426199Z","iopub.execute_input":"2022-09-07T20:32:35.426495Z","iopub.status.idle":"2022-09-07T20:32:35.434431Z","shell.execute_reply.started":"2022-09-07T20:32:35.426462Z","shell.execute_reply":"2022-09-07T20:32:35.433649Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"That's it -- we're now ready to train our model!","metadata":{}},{"cell_type":"code","source":"coeffs = train_model(lr=1.4)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.435750Z","iopub.execute_input":"2022-09-07T20:32:35.436537Z","iopub.status.idle":"2022-09-07T20:32:35.464318Z","shell.execute_reply.started":"2022-09-07T20:32:35.436501Z","shell.execute_reply":"2022-09-07T20:32:35.463518Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; ","output_type":"stream"}]},{"cell_type":"markdown","source":"It's looking good -- our loss is lower than before. Let's see if that translates to a better result on the validation set:","metadata":{}},{"cell_type":"code","source":"acc(coeffs)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.465885Z","iopub.execute_input":"2022-09-07T20:32:35.466128Z","iopub.status.idle":"2022-09-07T20:32:35.473081Z","shell.execute_reply.started":"2022-09-07T20:32:35.466095Z","shell.execute_reply":"2022-09-07T20:32:35.472032Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"tensor(0.7921)"},"metadata":{}}]},{"cell_type":"markdown","source":"In this case our neural net isn't showing better results than the linear model. That's not surprising; this dataset is very small and very simple, and isn't the kind of thing we'd expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!","metadata":{}},{"cell_type":"markdown","source":"## Deep learning","metadata":{}},{"cell_type":"markdown","source":"The neural net in the previous section only uses one hidden layer, so it doesn't count as \"deep\" learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\n\nFirst, we'll need to create additional coefficients for each layer:","metadata":{}},{"cell_type":"code","source":"def init_coeffs():\n    hiddens = [10, 10]  # <-- Two hidden layers, 10 activations (aka 10 column vectors of coefficents) in each. The\n                        # len of hiddens is the number of Epochs.\n                        # Each number in this array is: one round (epoch) of matrix multiplication where we are multiplying the\n                        # matrix of coefficients (each column vector is one set of coefficients, aka one activation)\n                        # and each number in the array, is the length (n_hidden) of the matrix we make with n_coeff by n_hidden\n                        # matrix we make, it can be an arbitrary size\n    sizes = [n_coeff] + hiddens + [1] # Sizes of each layer, first will go from n_coeffs to 10, second layer will go from 10 to 10, third will go from 10 to 1.\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.474983Z","iopub.execute_input":"2022-09-07T20:32:35.475637Z","iopub.status.idle":"2022-09-07T20:32:35.482555Z","shell.execute_reply.started":"2022-09-07T20:32:35.475541Z","shell.execute_reply":"2022-09-07T20:32:35.481861Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"The term \"hidden layers\" and \"layer\" both seem to be used for a couple purposes:\n\nLayer:\n- Each individual activation\n- Each individual epoch\n\nHidden Layers:\n- The number of activations in each layer\n- The number of epochs","metadata":{}},{"cell_type":"markdown","source":"You'll notice here that there's a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you'll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days -- it's very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we'll learn about in other notebooks.\n\nOur deep learning `calculate_predictions` looks much the same as before, but now we loop through each layer, instead of listing them separately:","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef calculate_predictions(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.485560Z","iopub.execute_input":"2022-09-07T20:32:35.486194Z","iopub.status.idle":"2022-09-07T20:32:35.495017Z","shell.execute_reply.started":"2022-09-07T20:32:35.486063Z","shell.execute_reply":"2022-09-07T20:32:35.494322Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"We also need a minor update to `update_coeffs` since we've got `layers` and `consts` separated now:","metadata":{}},{"cell_type":"code","source":"def update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.496384Z","iopub.execute_input":"2022-09-07T20:32:35.496873Z","iopub.status.idle":"2022-09-07T20:32:35.505354Z","shell.execute_reply.started":"2022-09-07T20:32:35.496835Z","shell.execute_reply":"2022-09-07T20:32:35.504593Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"Let's train our model...","metadata":{}},{"cell_type":"code","source":"coeffs = train_model(lr=4)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.506710Z","iopub.execute_input":"2022-09-07T20:32:35.507232Z","iopub.status.idle":"2022-09-07T20:32:35.540305Z","shell.execute_reply.started":"2022-09-07T20:32:35.507183Z","shell.execute_reply":"2022-09-07T20:32:35.539539Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; ","output_type":"stream"}]},{"cell_type":"markdown","source":"...and check its accuracy:","metadata":{}},{"cell_type":"code","source":"acc(coeffs)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T20:32:35.541794Z","iopub.execute_input":"2022-09-07T20:32:35.542045Z","iopub.status.idle":"2022-09-07T20:32:35.548727Z","shell.execute_reply.started":"2022-09-07T20:32:35.542013Z","shell.execute_reply":"2022-09-07T20:32:35.547851Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"tensor(0.8258)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Final thoughts","metadata":{}},{"cell_type":"markdown","source":"It's actually pretty cool that we've managed to create a real deep learning model from scratch and trained it to get over 80% accuracy on this task, all in the course of a single notebook!\n\nThe \"real\" deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you'll recognise the basic steps are the same.\n\nThe biggest differences in practical models to what we have above are:\n\n- How initialization and normalization is done to ensure the model trains correctly every time\n- Regularization (to avoid over-fitting)\n- Modifying the neural net itself to take advantage of knowledge of the problem domain\n- Doing gradient descent steps on smaller batches, rather than the whole dataset.","metadata":{}},{"cell_type":"markdown","source":"The neural net and deep learning models didn't make much of a difference. This was because this data was small and tabular.\nGenerally, you can chuck a deep learning model at image and NLP problems and get a great result. But tabular data isn't so easy.\nPre-trained models don't exist for tabular data because they are all unique (some do exist however) and you have to think about\nthe feature engineering for the data, a lot more.","metadata":{}}]}